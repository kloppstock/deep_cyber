{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "from tensorflow.keras import backend as K\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "## create_random_input\n",
    "\n",
    "Creates a random input vector in the desired shape. The numbers are between 0 and size / 128.\n",
    "\n",
    "**Input Parameters**\n",
    "* shape -- The desired shape\n",
    "* divider -- The divider to scale the output by\n",
    "\n",
    "**Output Parameters**\n",
    "* The desired tensor\n",
    "\n",
    "## ref_conf2d\n",
    "\n",
    "The reference Conv2D implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "* w -- The weights\n",
    "* b -- The bias\n",
    "* kernel_size -- The kernel size (kernel_rows, kernel_cols)\n",
    "* strides -- The strides (stride_rows, stride_cols)\n",
    "* padding -- \"valid\" for no padding, \"same\" for zero padding\n",
    "* groups -- The number of desired groups\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the convolution\n",
    "\n",
    "## check_conv2d\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* shape -- The shape to test the function with\n",
    "* kernel_size -- The kernel size (kernel_rows, kernel_cols) \n",
    "* filters -- The number of filters to use\n",
    "* strides -- The strides (stride_rows, stride_cols)\n",
    "* padding -- \"valid\" for no padding, \"same\" for zero padding\n",
    "* groups -- The number of desired groups\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise\n",
    "\n",
    "## Important Note\n",
    "\n",
    "Due to floating point inaccuracies (hopefully), the results are not equal but close instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_input(shape, divider=128.0):\n",
    "    num_elements = np.prod(shape)\n",
    "    return np.random.randint(0, num_elements, num_elements).astype(np.float32).reshape(shape) / divider\n",
    "\n",
    "def ref_conv2d(X, w, b, kernel_size, strides, padding, groups):\n",
    "    return Conv2D(input_shape=X.shape, weights=[w, b], kernel_size=kernel_size, filters=w.shape[3], strides=strides, padding=padding, groups=groups)(X).numpy()\n",
    "    \n",
    "def check_conv2d(fn, shape, kernel_size, filters, strides, padding, groups=1):\n",
    "    np.random.seed(3)\n",
    "    X = create_random_input(shape)\n",
    "    w = create_random_input((kernel_size[0], kernel_size[1], shape[3], filters))\n",
    "    b = create_random_input((filters,))\n",
    "    \n",
    "    ref = ref_conv2d(X, w, b, kernel_size, strides, padding, groups)\n",
    "    res = fn(X, w, b, kernel_size, strides, padding, groups)\n",
    "    \n",
    "    return np.allclose(res, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2D Implementation\n",
    "\n",
    "The actual Conv2D implementation. This function has the same input and output parameters as the ref_conv2d function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def res_conv2d(X, w, b, kernel_size, strides, padding, groups=1):\n",
    "    # extract parameters\n",
    "    (kernel_rows, kernel_cols, channels_in, channels_out) = w.shape\n",
    "    (batch, rows_in, cols_in, channels_in_) = X.shape\n",
    "    grouped_channels_out = channels_out//groups\n",
    "    \n",
    "    # check parameter compatibility\n",
    "    assert channels_in * groups == channels_in_\n",
    "    assert channels_out % groups == 0\n",
    "    \n",
    "    # same padding\n",
    "    if padding == \"same\":\n",
    "        rows_offset = kernel_rows//2\n",
    "        cols_offset = kernel_cols//2\n",
    "    \n",
    "        # calculate output dimensions\n",
    "        rows_out = (rows_in + strides[0] - 1) // strides[0]\n",
    "        cols_out = (cols_in + strides[1] - 1) // strides[1]\n",
    "        \n",
    "    \n",
    "        # create output buffer\n",
    "        out = np.zeros((int(batch), int(rows_out), int(cols_out), int(groups*channels_out)))\n",
    "\n",
    "        # prefill the output with bias\n",
    "        for i in range(batch):\n",
    "            for y in range(rows_out):\n",
    "                for x in range(cols_out):\n",
    "                    for g in range(groups):\n",
    "                        gc = g * grouped_channels_out\n",
    "                        for co in range(grouped_channels_out):\n",
    "                            out[i, y, x, gc + co] = b[co]\n",
    "\n",
    "        # convolute\n",
    "        for i in range(batch):\n",
    "            for y in range(rows_out):\n",
    "                for x in range(cols_out):\n",
    "                    sy = y * strides[0] - rows_offset\n",
    "                    sx = x * strides[1] - rows_offset\n",
    "                    for g in range(groups):\n",
    "                        for co in range(grouped_channels_out):\n",
    "                            for ci in range(channels_in):\n",
    "                                for ky in range(sy, sy + kernel_rows):\n",
    "                                    for kx in range(sx, sx + kernel_cols):\n",
    "                                        if ky >= 0 and ky < rows_in and kx >= 0 and kx < cols_in:\n",
    "                                            gc = g * grouped_channels_out\n",
    "                                            out[i, y, x, gc + co] += X[i, ky , kx, gc + ci] * w[(ky - sy), (kx - sx), ci, co]\n",
    "\n",
    "        return out\n",
    "    \n",
    "    # zero padding\n",
    "    else:\n",
    "        rows_out = (rows_in - (kernel_rows - strides[0])) // strides[0]\n",
    "        cols_out = (cols_in - (kernel_cols - strides[1])) // strides[1]\n",
    "        \n",
    "    \n",
    "        # create output buffer\n",
    "        out = np.zeros((int(batch), int(rows_out), int(cols_out), int(groups*channels_out)))\n",
    "\n",
    "        # prefill the output with bias\n",
    "        for i in range(batch):\n",
    "            for y in range(rows_out):\n",
    "                for x in range(cols_out):\n",
    "                    for g in range(groups):\n",
    "                        gc = g * grouped_channels_out\n",
    "                        for co in range(grouped_channels_out):\n",
    "                            out[i, y, x, gc + co] = b[co]\n",
    "\n",
    "        # convolute\n",
    "        for i in range(batch):\n",
    "            for y in range(rows_out):\n",
    "                for x in range(cols_out):\n",
    "                    sy = y * strides[0]\n",
    "                    sx = x * strides[1]\n",
    "                    for g in range(groups):\n",
    "                        for co in range(grouped_channels_out):\n",
    "                            for ci in range(channels_in):\n",
    "                                for ky in range(sy, sy + kernel_rows):\n",
    "                                    for kx in range(sx, sx + kernel_cols):\n",
    "                                        gc = g * grouped_channels_out\n",
    "                                        out[i, y, x, gc + co] += X[i, ky , kx, gc + ci] * w[(ky - sy), (kx - sx), ci, co]\n",
    "\n",
    "        return out\n",
    "\n",
    "print(check_conv2d(res_conv2d, (2, 128, 128, 4), (3, 3), 5, (1, 1), \"same\"))\n",
    "#print(check_conv2d(res_conv2d, (1, 3, 3, 1), (3, 3), 1, (1, 1), \"valid\"))\n",
    "#print(check_conv2d(res_conv2d, (1, 4, 4, 1), (1, 1), 1, (1, 1), \"valid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Helper Functions\n",
    "\n",
    "## ref_dense\n",
    "\n",
    "The reference Dense implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "* w -- The weights\n",
    "* b -- The bias\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the convolution\n",
    "\n",
    "## check_dense\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* units -- The dimension of the output\n",
    "* shape -- The shape to test the function with\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise\n",
    "\n",
    "## Important Note\n",
    "\n",
    "Due to floating point inaccuracies (hopefully), the results are not equal but close instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_dense(X, w, b):\n",
    "    return Dense(w.shape[1], input_shape=X.shape, weights=[w, b])(X).numpy()\n",
    "    \n",
    "def check_dense(fn, units, shape):\n",
    "    np.random.seed(3)\n",
    "    shape = np.array(shape)\n",
    "    size = np.prod(shape[1:])\n",
    "    \n",
    "    X = create_random_input((shape[0], size))\n",
    "    w = create_random_input((size, units))\n",
    "    b = create_random_input((units))\n",
    "    \n",
    "    ref = ref_dense(X, w, b)\n",
    "    res = fn(X, w, b)\n",
    "    \n",
    "    #return (res == ref).all()\n",
    "    return np.allclose(res, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Layer Implementation\n",
    "\n",
    "The actual dense layer implementation. This function has the same input and output parameters as the ref_dense function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def res_dense(X, w, b):\n",
    "    # extract parameters\n",
    "    (batches, size) = X.shape\n",
    "    (w_s, units) = w.shape\n",
    "    \n",
    "    # check parameter compatibility\n",
    "    assert size == w_s\n",
    "    \n",
    "    # create output buffer\n",
    "    out = np.zeros((batches, units))\n",
    "        \n",
    "    # prefill output with bias\n",
    "    for i in range(batches):\n",
    "        for j in range(units):\n",
    "            out[i, j] = b[j]\n",
    "    \n",
    "    # matrix multiplikation\n",
    "    for i in range(batches):\n",
    "        for j in range(units):\n",
    "            for k in range(size):\n",
    "                out[i, j] += X[i, k] * w[k, j]\n",
    "                \n",
    "    return out\n",
    "\n",
    "print(check_dense(res_dense, 3, (5, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even More Helper Functions\n",
    "\n",
    "## ref_relu\n",
    "\n",
    "The reference ReLU implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the convolution\n",
    "\n",
    "## check_relu\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* shape -- The shape to test the function with\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise\n",
    "\n",
    "## ref_sigmoid\n",
    "\n",
    "The reference sigmoid implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the convolution\n",
    "\n",
    "## check_sigmoid\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* shape -- The shape to test the function with\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise\n",
    "\n",
    "## ref_softmax\n",
    "\n",
    "The reference softmax implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the convolution\n",
    "\n",
    "## check_softmax\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* shape -- The shape to test the function with\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise\n",
    "\n",
    "\n",
    "## Important Note\n",
    "\n",
    "Due to floating point inaccuracies (hopefully), the results of the sigmoid functions are not equal but close instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_relu(X):\n",
    "    return relu(X).numpy()\n",
    "    \n",
    "def check_relu(fn, shape):\n",
    "    np.random.seed(3)\n",
    "    shape = np.array(shape)\n",
    "    \n",
    "    X = create_random_input(shape)\n",
    "    \n",
    "    ref = ref_relu(X)\n",
    "    res = fn(X)\n",
    "    \n",
    "    return (res == ref).all()\n",
    "    #return np.allclose(res, ref)\n",
    "    \n",
    "def ref_sigmoid(X):\n",
    "    return sigmoid(X).numpy()\n",
    "    \n",
    "def check_sigmoid(fn, shape):\n",
    "    np.random.seed(3)\n",
    "    shape = np.array(shape)\n",
    "    \n",
    "    X = create_random_input(shape)\n",
    "    \n",
    "    ref = ref_sigmoid(X)\n",
    "    res = fn(X)\n",
    "    \n",
    "    #return (res == ref).all()\n",
    "    return np.allclose(res, ref)\n",
    "    \n",
    "def ref_softmax(X):\n",
    "    # softmax(K.constant(X)).numpy()\n",
    "    return softmax(K.constant(X)).numpy()\n",
    "    \n",
    "def check_softmax(fn, shape):\n",
    "    np.random.seed(3)\n",
    "    shape = np.array(shape)\n",
    "    \n",
    "    X = create_random_input(shape, np.prod(shape))\n",
    "    \n",
    "    ref = ref_softmax(X)\n",
    "    res = fn(X)\n",
    "    \n",
    "    #return (res == ref).all()\n",
    "    return np.allclose(res, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Layer Implementation\n",
    "\n",
    "The actual relu, sigmoid and softmax activation layer implementations. These function have the same input and output parameters as the ref_relu, ref_sigmoid and ref_softmax functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def res_relu(X):\n",
    "    # extract parameters\n",
    "    shape = X.shape\n",
    "    X = X.flatten()\n",
    "    \n",
    "    # create output buffer\n",
    "    out = np.zeros(X.shape)\n",
    "    \n",
    "    # activate all cells\n",
    "    for i in range(X.shape[0]):\n",
    "        out[i] = 0 if X[i] <= 0. else X[i]\n",
    "        \n",
    "    return out.reshape(shape)\n",
    "\n",
    "def res_sigmoid(X):\n",
    "    # extract parameters\n",
    "    shape = X.shape\n",
    "    X = X.flatten()\n",
    "    e = 2.7182818284590452353602874\n",
    "    \n",
    "    # create output buffer\n",
    "    out = np.zeros(X.shape)\n",
    "    \n",
    "    # activate all cells\n",
    "    for i in range(X.shape[0]):\n",
    "        out[i] = 1 / (1 + e**(-X[i]))\n",
    "        \n",
    "    return out.reshape(shape)\n",
    "\n",
    "def res_softmax(X):\n",
    "    # extract parameters\n",
    "    (batches, cells) = X.shape\n",
    "    e = 2.7182818284590452353602874\n",
    "    \n",
    "    # create output buffer\n",
    "    out = np.zeros(X.shape)\n",
    "    \n",
    "    # sum of exponents\n",
    "    expsums = np.zeros(batches)\n",
    "    for i in range(batches):\n",
    "        for j in range(cells):\n",
    "            expsums[i] += e**X[i, j]\n",
    "    \n",
    "    # activate all cells\n",
    "    for i in range(batches):\n",
    "        for j in range(cells):\n",
    "            out[i, j] = e**X[i, j] / expsums[i]\n",
    "        \n",
    "    return out\n",
    "\n",
    "print(check_relu(res_relu, (5, 256)))\n",
    "print(check_sigmoid(res_sigmoid, (5, 256)))\n",
    "print(check_softmax(res_softmax, (100, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Still Even More Helper Functions\n",
    "\n",
    "## ref_maxpool2d\n",
    "\n",
    "The reference max pooling implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "* pool_size -- The size of the pool (pool_rows, pool_cols)\n",
    "* strides -- The strides (stride_rows, stride_cols)\n",
    "* padding -- \"valid\" for no padding, \"same\" for zero padding\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the pooling\n",
    "\n",
    "## check_maxpool2d\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* shape -- The shape to test the function with\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise\n",
    "\n",
    "## ref_avgpool2d\n",
    "\n",
    "The reference average pooling implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "* pool_size -- The size of the pool (pool_rows, pool_cols)\n",
    "* strides -- The strides (stride_rows, stride_cols)\n",
    "* padding -- \"valid\" for no padding, \"same\" for zero padding\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the convolution\n",
    "\n",
    "## check_avgpool2d\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* shape -- The shape to test the function with\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_maxpool2d(X, pool_size, strides, padding):\n",
    "    return MaxPooling2D(pool_size=pool_size, strides=strides, padding=padding)(X).numpy()\n",
    "    \n",
    "def check_maxpool2d(fn, shape, pool_size, strides, padding):\n",
    "    np.random.seed(3)\n",
    "    shape = np.array(shape)\n",
    "    \n",
    "    X = create_random_input(shape)\n",
    "    \n",
    "    ref = ref_maxpool2d(X, pool_size, strides, padding)\n",
    "    res = fn(X, pool_size, strides, padding)\n",
    "    \n",
    "    return (res == ref).all()\n",
    "    \n",
    "def ref_avgpool2d(X, pool_size, strides, padding):\n",
    "    return AveragePooling2D(pool_size=pool_size, strides=strides, padding=padding)(X).numpy()\n",
    "    \n",
    "def check_avgpool2d(fn, shape, pool_size, strides, padding):\n",
    "    np.random.seed(3)\n",
    "    shape = np.array(shape)\n",
    "    \n",
    "    X = create_random_input(shape)\n",
    "    \n",
    "    ref = ref_avgpool2d(X, pool_size, strides, padding)\n",
    "    res = fn(X, pool_size, strides, padding)\n",
    "    \n",
    "    return np.allclose(res, ref)\n",
    "    return (res == ref).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling Layer Implementation\n",
    "\n",
    "The actual maxpool2d and avgpool2d activation layer implementations. These function have the same input and output parameters as the ref_maxpool2d and ref_avgpool2d functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def res_maxpool2d(X, pool_sizes, strides, padding):\n",
    "    # extract parameters\n",
    "    (batch, rows_in, cols_in, channels) = X.shape\n",
    "    (pool_rows, pool_cols) = pool_sizes\n",
    "    (stride_rows, stride_cols) = strides\n",
    "    \n",
    "    # same padding\n",
    "    if padding == \"same\":    \n",
    "        # calculate output dimensions\n",
    "        rows_out = (rows_in + strides[0] - 1) // strides[0]\n",
    "        cols_out = (cols_in + strides[1] - 1) // strides[1]\n",
    "        \n",
    "        # create output buffer\n",
    "        out = np.zeros((batch, rows_out, cols_out, channels))\n",
    "        out.fill(sys.float_info.min)\n",
    "\n",
    "        # pool\n",
    "        for i in range(batch):\n",
    "            for y in range(rows_out):\n",
    "                for x in range(cols_out):\n",
    "                    sy = y * strides[0]\n",
    "                    sx = x * strides[1]\n",
    "                    for c in range(channels):\n",
    "                        for ky in range(sy, sy + pool_rows):\n",
    "                            for kx in range(sx, sx + pool_cols):\n",
    "                                if ky >= 0 and ky < rows_in and kx >= 0 and kx < cols_in:\n",
    "                                    out[i, y, x, c] = max(out[i, y, x, c], X[i, ky , kx, c])\n",
    "\n",
    "        return out\n",
    "    \n",
    "    # zero padding\n",
    "    else:\n",
    "        rows_out = (rows_in - (pool_rows - strides[0])) // strides[0]\n",
    "        cols_out = (cols_in - (pool_cols - strides[1])) // strides[1]\n",
    "        \n",
    "    \n",
    "        # create output buffer\n",
    "        out = np.zeros((batch, rows_out, cols_out, channels))\n",
    "\n",
    "        # pool\n",
    "        for i in range(batch):\n",
    "            for y in range(rows_out):\n",
    "                for x in range(cols_out):\n",
    "                    sy = y * strides[0]\n",
    "                    sx = x * strides[1]\n",
    "                    for c in range(channels):\n",
    "                        for ky in range(sy, sy + pool_rows):\n",
    "                            for kx in range(sx, sx + pool_cols):\n",
    "                                out[i, y, x, c] = max(out[i, y, x, c], X[i, ky , kx, c])\n",
    "\n",
    "        return out\n",
    "    \n",
    "print(check_maxpool2d(res_maxpool2d, (2, 128, 128, 4), (2, 3), (4, 5), \"same\"))\n",
    "print(check_maxpool2d(res_maxpool2d, (2, 128, 128, 4), (2, 3), (4, 5), \"valid\"))\n",
    "\n",
    "def res_avgpool2d(X, pool_sizes, strides, padding):\n",
    "    # extract parameters\n",
    "    (batch, rows_in, cols_in, channels) = X.shape\n",
    "    (pool_rows, pool_cols) = pool_sizes\n",
    "    (stride_rows, stride_cols) = strides\n",
    "    \n",
    "    # same padding\n",
    "    if padding == \"same\":    \n",
    "        # calculate output dimensions\n",
    "        rows_out = (rows_in + strides[0] - 1) // strides[0]\n",
    "        cols_out = (cols_in + strides[1] - 1) // strides[1]\n",
    "        \n",
    "        # create output buffer\n",
    "        out = np.zeros((batch, rows_out, cols_out, channels))\n",
    "\n",
    "        # pool\n",
    "        for i in range(batch):\n",
    "            for y in range(rows_out):\n",
    "                for x in range(cols_out):\n",
    "                    sy = y * strides[0]\n",
    "                    sx = x * strides[1]\n",
    "                    for c in range(channels):\n",
    "                        area = 0\n",
    "                        for ky in range(sy, sy + pool_rows):\n",
    "                            for kx in range(sx, sx + pool_cols):\n",
    "                                if ky >= 0 and ky < rows_in and kx >= 0 and kx < cols_in:\n",
    "                                    out[i, y, x, c] += X[i, ky , kx, c]\n",
    "                                    area += 1\n",
    "                        out[i, y, x, c] /= area\n",
    "\n",
    "        return out\n",
    "    \n",
    "    # zero padding\n",
    "    else:\n",
    "        rows_out = (rows_in - (pool_rows - strides[0])) // strides[0]\n",
    "        cols_out = (cols_in - (pool_cols - strides[1])) // strides[1]\n",
    "        \n",
    "    \n",
    "        # create output buffer\n",
    "        out = np.zeros((batch, rows_out, cols_out, channels))\n",
    "\n",
    "        # pool\n",
    "        for i in range(batch):\n",
    "            for y in range(rows_out):\n",
    "                for x in range(cols_out):\n",
    "                    sy = y * strides[0]\n",
    "                    sx = x * strides[1]\n",
    "                    for c in range(channels):\n",
    "                        for ky in range(sy, sy + pool_rows):\n",
    "                            for kx in range(sx, sx + pool_cols):\n",
    "                                out[i, y, x, c] += X[i, ky , kx, c]\n",
    "                        out[i, y, x, c] /= pool_rows * pool_cols\n",
    "\n",
    "        return out\n",
    " \n",
    "print(check_avgpool2d(res_avgpool2d, (2, 128, 128, 4), (2, 3), (4, 5), \"same\"))\n",
    "print(check_avgpool2d(res_avgpool2d, (2, 128, 128, 4), (2, 3), (4, 5), \"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
