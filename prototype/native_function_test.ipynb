{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "from tensorflow.keras import backend as K\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from ctypes import CDLL, Structure, c_uint, c_float, c_ubyte, POINTER, pointer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Structure Wrapper\n",
    "\n",
    "## unpackIndex\n",
    "\n",
    "Unpax a tuple into a 4-tuple and fills all missing values with the default. \n",
    "\n",
    "**Input Parameters**\n",
    "* i -- The desired shape\n",
    "* default -- The default value\n",
    "\n",
    "**Output Parameters**\n",
    "* The desired shape\n",
    "\n",
    "## Tensor\n",
    "\n",
    "The Tensor wrapper class. \n",
    "\n",
    "### __init__\n",
    "\n",
    "The constructor. Either creates a tensor from a numpy array or a shape. \n",
    "\n",
    "**Input Parameters**\n",
    "* i -- The input (either a shape tuple or a numpy array)\n",
    "\n",
    "### __del__\n",
    "\n",
    "The destructor. Frees the internally allocated memory. \n",
    "\n",
    "### __getitem__\n",
    "\n",
    "Array getter. \n",
    "\n",
    "**Input Parameters:**\n",
    "* i -- The index tuple\n",
    "\n",
    "**Output Parameters**\n",
    "* The requested element\n",
    "\n",
    "### __setitem__\n",
    "\n",
    "Array setter. \n",
    "\n",
    "**Input Parameters:**\n",
    "* i -- The index tuple\n",
    "\n",
    "### shape\n",
    "\n",
    "Returns the shape tuple. \n",
    "\n",
    "**Output Parameters**\n",
    "* The shape tuple\n",
    "\n",
    "### reshape\n",
    "\n",
    "Reshapes the Tensor. \n",
    "\n",
    "**Input Parameters:**\n",
    "* i -- The new shape tuple\n",
    "\n",
    "## numpy\n",
    "\n",
    "Converts the tensor to a numpy array\n",
    "\n",
    "**Output Parameters**\n",
    "* The numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = CDLL(\"./libdeep_cyber.so\")\n",
    "\n",
    "def unpackIndex(i, default):\n",
    "    a = b = c = d = default\n",
    "    if type(i) == int:\n",
    "        d = i\n",
    "    elif len(i) == 1:\n",
    "        d = i[0]\n",
    "    elif len(i) == 2:\n",
    "        c = i[0]\n",
    "        d = i[1]\n",
    "    elif len(i) == 3:\n",
    "        b = i[0]\n",
    "        c = i[1]\n",
    "        d = i[2]\n",
    "    else:\n",
    "        a = i[0]\n",
    "        b = i[1]\n",
    "        c = i[2]\n",
    "        d = i[3]\n",
    "    return (a, b, c, d)\n",
    "\n",
    "class Tensor(Structure):\n",
    "    _fields_ = [(\"a\", c_uint),\n",
    "               (\"b\", c_uint),\n",
    "               (\"c\", c_uint),\n",
    "               (\"d\", c_uint),\n",
    "               (\"data\", POINTER(c_float))]\n",
    "\n",
    "    def __init__(self, i):\n",
    "        if type(i) == tuple:\n",
    "            (a, b, c, d) = unpackIndex(i, 1)\n",
    "            lib.create_tensor.argtypes = [c_uint, c_uint, c_uint, c_uint]\n",
    "            lib.create_tensor.restype = Tensor\n",
    "            t = lib.create_tensor(a, b, c, d)\n",
    "            self.a = t.a\n",
    "            self.b = t.b\n",
    "            self.c = t.c\n",
    "            self.d = t.d\n",
    "            self.data = t.data\n",
    "            t.data = None\n",
    "        elif type(i) == np.ndarray:\n",
    "            # create new tensor\n",
    "            (a, b, c, d) = unpackIndex(i.shape, 1)\n",
    "            lib.create_tensor.argtypes = [c_uint, c_uint, c_uint, c_uint]\n",
    "            lib.create_tensor.restype = Tensor\n",
    "            \n",
    "            # copy data from numpy array\n",
    "            t = lib.create_tensor(1, 1, 1, a*b*c*d)\n",
    "            i = i.flatten()\n",
    "            for x in range(t.d):\n",
    "                t[x] = float(i[x])\n",
    "                \n",
    "            # move data \n",
    "            self.data = t.data\n",
    "            t.data = None\n",
    "            self.a = a\n",
    "            self.b = b\n",
    "            self.c = c\n",
    "            self.d = d\n",
    "        else:\n",
    "            raise(\"Illegal input type!\")\n",
    "\n",
    "    def __del__(self):\n",
    "        lib.free_tensor.argtypes = [Tensor]\n",
    "        lib.free_tensor(self)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        (a, b, c, d) = unpackIndex(i, 0)\n",
    "        lib.at.argtypes = [POINTER(Tensor), c_uint, c_uint, c_uint, c_uint]\n",
    "        lib.at.restype = POINTER(c_float)\n",
    "        return lib.at(pointer(self), a, b, c, d)[0]\n",
    "\n",
    "    def __setitem__(self, i, v):\n",
    "        (a, b, c, d) = unpackIndex(i, 0)\n",
    "        lib.at.argtypes = [POINTER(Tensor), c_uint, c_uint, c_uint, c_uint]\n",
    "        lib.at.restype = POINTER(c_float)\n",
    "        lib.at(pointer(self), a, b, c, d)[0] = float(v)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        if self.a != 1 and self.b != 1 and self.c != 1 and self.d != 1:\n",
    "            return (self.a, self.b, self.c, self.d)\n",
    "        elif self.b != 1 and self.c != 1 and self.d != 1:\n",
    "            return (self.b, self.c, self.d)\n",
    "        elif self.c != 1 and self.d != 1:\n",
    "            return (self.c, self.d)\n",
    "        else:\n",
    "            return (self.d, )\n",
    "        \n",
    "    def reshape(self, i):\n",
    "        (self.a, self.b, self.c, self.d) = unpackIndex(i, 1)\n",
    "        \n",
    "    def numpy(self):\n",
    "        t = np.zeros((self.a, self.b, self.c, self.d))\n",
    "        for ai in range(self.a):\n",
    "            for bi in range(self.b):\n",
    "                for ci in range(self.c):\n",
    "                    for di in range(self.d):\n",
    "                        t[ai, bi, ci, di] = self[ai, bi, ci, di]\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "## create_random_input\n",
    "\n",
    "Creates a random input vector in the desired shape. The numbers are between 0 and size / 128.\n",
    "\n",
    "**Input Parameters**\n",
    "* shape -- The desired shape\n",
    "* divider -- The divider to scale the output by\n",
    "\n",
    "**Output Parameters**\n",
    "* The desired tensor\n",
    "\n",
    "## ref_conf2d\n",
    "\n",
    "The reference Conv2D implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "* w -- The weights\n",
    "* b -- The bias\n",
    "* kernel_size -- The kernel size (kernel_rows, kernel_cols)\n",
    "* strides -- The strides (stride_rows, stride_cols)\n",
    "* padding -- \"valid\" for no padding, \"same\" for zero padding\n",
    "* groups -- The number of desired groups\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the convolution\n",
    "\n",
    "## check_conv2d\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* shape -- The shape to test the function with\n",
    "* kernel_size -- The kernel size (kernel_rows, kernel_cols) \n",
    "* filters -- The number of filters to use\n",
    "* strides -- The strides (stride_rows, stride_cols)\n",
    "* padding -- \"valid\" for no padding, \"same\" for zero padding\n",
    "* groups -- The number of desired groups\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise\n",
    "\n",
    "## Important Note\n",
    "\n",
    "Due to floating point inaccuracies (hopefully), the results are not equal but close instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_input(shape, divider=128.0):\n",
    "    num_elements = np.prod(shape)\n",
    "    return np.random.randint(0, num_elements, num_elements).astype(np.float32).reshape(shape) / divider\n",
    "\n",
    "def ref_conv2d(X, w, b, kernel_size, strides, padding, groups):\n",
    "    return Conv2D(input_shape=X.shape, weights=[w, b], kernel_size=kernel_size, filters=w.shape[3], strides=strides, padding=padding, groups=groups)(X).numpy()\n",
    "    \n",
    "def check_conv2d(fn, shape, kernel_size, filters, strides, padding, groups=1):\n",
    "    np.random.seed(3)\n",
    "    X = create_random_input(shape)\n",
    "    w = create_random_input((kernel_size[0], kernel_size[1], shape[3], filters))\n",
    "    b = create_random_input((filters,))\n",
    "    \n",
    "    ref = ref_conv2d(X, w, b, kernel_size, strides, padding, groups)\n",
    "    res = fn(X, w, b, kernel_size, strides, padding, groups)\n",
    "    \n",
    "    #diff = (ref - res).reshape(res.shape[1], res.shape[2],)\n",
    "    diff = ref - res\n",
    "    \n",
    "    print(diff.shape)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(diff[0, 0:-1, 0:-1, 0])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(diff[0, 0:-1, 0:-1, 1])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(diff[1, 0:-1, 0:-1, 0])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(diff[1, 0:-1, 0:-1, 1])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    return np.allclose(res, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2D Implementation\n",
    "\n",
    "The actual Conv2D implementation. This function has the same input and output parameters as the ref_conv2d function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_conv2d(X, w, b, kernel_size, strides, padding, groups):\n",
    "    X = Tensor(X)\n",
    "    w = Tensor(w)\n",
    "    b = Tensor(b)\n",
    "    lib.conv2d.argtypes = [Tensor, Tensor, Tensor, c_uint, c_uint, c_ubyte, c_uint]\n",
    "    lib.conv2d.restype = Tensor\n",
    "    return lib.conv2d(X, w, b, int(strides[0]), int(strides[1]), int(padding == \"same\"), int(groups)).numpy()\n",
    "    \n",
    "#print(check_conv2d(res_conv2d, (2, 128, 128, 2), (3, 3), 2, (1, 1), \"valid\"))\n",
    "#print(check_conv2d(res_conv2d, (2, 128, 128, 4), (3, 3), 5, (3, 3), \"same\"))\n",
    "#print(check_conv2d(res_conv2d, (1, 3, 3, 1), (3, 3), 1, (1, 1), \"valid\"))\n",
    "#print(check_conv2d(res_conv2d, (1, 4, 4, 1), (1, 1), 1, (1, 1), \"valid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Helper Functions\n",
    "\n",
    "## ref_dense\n",
    "\n",
    "The reference Dense implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "* w -- The weights\n",
    "* b -- The bias\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the convolution\n",
    "\n",
    "## check_dense\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* units -- The dimension of the output\n",
    "* shape -- The shape to test the function with\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise\n",
    "\n",
    "## Important Note\n",
    "\n",
    "Due to floating point inaccuracies (hopefully), the results are not equal but close instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_dense(X, w, b):\n",
    "    return Dense(w.shape[1], input_shape=X.shape, weights=[w, b])(X).numpy()\n",
    "    \n",
    "def check_dense(fn, units, shape):\n",
    "    np.random.seed(3)\n",
    "    shape = np.array(shape)\n",
    "    size = np.prod(shape[1:])\n",
    "    \n",
    "    X = create_random_input((shape[0], size))\n",
    "    w = create_random_input((size, units))\n",
    "    b = create_random_input((units))\n",
    "    \n",
    "    ref = ref_dense(X, w, b)\n",
    "    res = fn(X, w, b)\n",
    "    \n",
    "    #return (res == ref).all()\n",
    "    return np.allclose(res, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Layer Implementation\n",
    "\n",
    "The actual dense layer implementation. This function has the same input and output parameters as the ref_dense function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def res_dense(X, w, b):\n",
    "    X = Tensor(X)\n",
    "    w = Tensor(w)\n",
    "    b = Tensor(b)\n",
    "    lib.dense.argtypes = [Tensor, Tensor, Tensor]\n",
    "    lib.dense.restype = Tensor\n",
    "    return lib.dense(X, w, b).numpy()\n",
    "\n",
    "print(check_dense(res_dense, 3, (5, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even More Helper Functions\n",
    "\n",
    "## ref_relu\n",
    "\n",
    "The reference ReLU implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the convolution\n",
    "\n",
    "## check_relu\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* shape -- The shape to test the function with\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise\n",
    "\n",
    "## ref_sigmoid\n",
    "\n",
    "The reference sigmoid implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the convolution\n",
    "\n",
    "## check_sigmoid\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* shape -- The shape to test the function with\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise\n",
    "\n",
    "## ref_softmax\n",
    "\n",
    "The reference softmax implementation from TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* X -- The input tensor\n",
    "\n",
    "\n",
    "**Output Parameters**\n",
    "* The result tensor of the convolution\n",
    "\n",
    "## check_softmax\n",
    "\n",
    "Checks an input function with the given parameters against the reference implementation of TensorFlow. \n",
    "\n",
    "**Input Parameters**\n",
    "* fn -- The function to compare to\n",
    "* shape -- The shape to test the function with\n",
    "\n",
    "**Output Parameters**\n",
    "* True if the function produces the same output as the reference, False otherwise\n",
    "\n",
    "\n",
    "## Important Note\n",
    "\n",
    "Due to floating point inaccuracies (hopefully), the results of the sigmoid functions are not equal but close instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_relu(X):\n",
    "    return relu(X).numpy()\n",
    "    \n",
    "def check_relu(fn, shape):\n",
    "    np.random.seed(3)\n",
    "    shape = np.array(shape)\n",
    "    \n",
    "    X = create_random_input(shape)\n",
    "    \n",
    "    ref = ref_relu(X)\n",
    "    res = fn(X)\n",
    "    \n",
    "    return (res == ref).all()\n",
    "    #return np.allclose(res, ref)\n",
    "    \n",
    "def ref_sigmoid(X):\n",
    "    return sigmoid(X).numpy()\n",
    "    \n",
    "def check_sigmoid(fn, shape):\n",
    "    np.random.seed(3)\n",
    "    shape = np.array(shape)\n",
    "    \n",
    "    X = create_random_input(shape)\n",
    "    \n",
    "    ref = ref_sigmoid(X)\n",
    "    res = fn(X)\n",
    "    \n",
    "    #return (res == ref).all()\n",
    "    return np.allclose(res, ref)\n",
    "    \n",
    "def ref_softmax(X):\n",
    "    # softmax(K.constant(X)).numpy()\n",
    "    return softmax(K.constant(X)).numpy()\n",
    "    \n",
    "def check_softmax(fn, shape):\n",
    "    np.random.seed(3)\n",
    "    shape = np.array(shape)\n",
    "    \n",
    "    X = create_random_input(shape, np.prod(shape))\n",
    "    \n",
    "    ref = ref_softmax(X)\n",
    "    res = fn(X)\n",
    "    \n",
    "    diff = (ref - res).reshape((shape, np.prod(shape)))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(diff)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    #return (res == ref).all()\n",
    "    return np.allclose(res, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Layer Implementation\n",
    "\n",
    "The actual relu, sigmoid and softmax activation layer implementations. These function have the same input and output parameters as the ref_relu, ref_sigmoid and ref_softmax functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def res_relu(X):\n",
    "    X = Tensor(X)\n",
    "    lib.relu.argtypes = [Tensor]\n",
    "    lib.relu.restype = Tensor\n",
    "    return lib.relu(X).numpy()\n",
    "\n",
    "def res_sigmoid(X):\n",
    "    X = Tensor(X)\n",
    "    lib.sigmoid.argtypes = [Tensor]\n",
    "    lib.sigmoid.restype = Tensor\n",
    "    return lib.sigmoid(X).numpy()\n",
    "\n",
    "def res_softmax(X):\n",
    "    X = Tensor(X)\n",
    "    lib.softmax.argtypes = [Tensor]\n",
    "    lib.softmax.restype = Tensor\n",
    "    return lib.softmax(X).numpy()\n",
    "\n",
    "print(check_relu(res_relu, (5, 256)))\n",
    "print(check_sigmoid(res_sigmoid, (5, 256)))\n",
    "print(check_softmax(res_softmax, (100, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
